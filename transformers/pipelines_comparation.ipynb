{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3cdfdba",
   "metadata": {},
   "source": [
    "### Homework：替换以上示例中的模型，对比不同模型在相同任务上的性能表现\n",
    "\n",
    "在 Hugging Face Models 中找到适合你的模型：https://huggingface.co/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d6ac9a",
   "metadata": {},
   "source": [
    "#### 1.设置环境变量，用于保存模型；设置网络代理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6296694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['HF_HOME'] = '/root/autodl-tmp/huggingface'\n",
    "os.environ['HF_HUB_CACHE'] = '/root/autodl-tmp/huggingface/hub'\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7845016d",
   "metadata": {},
   "source": [
    "#### 1.对比文本分类任务\n",
    "构造测试用数据，设置多个比对模型，依次调用，测试文本分类，分别对比各模型的实际效果。模型抽取了排名靠前的若干模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52a734af",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_texts = ['今天是一个阳光明媚的日子。',\n",
    " '这部电影太令人失望了，情节乏味，表演也很差。',\n",
    " '我喜欢和朋友一起度过愉快的时光。',\n",
    " '这家餐厅的食物味道很美味，服务也很周到。',\n",
    " '今天的工作任务很繁重，但我感到很充实。',\n",
    " '这本书的内容很有启发性，让我受益匪浅。',\n",
    " '我觉得这个问题很难回答，需要更多的思考。',\n",
    " '这个城市的环境优美，适合居住。',\n",
    " '对于这个方案，我持保留态度。',\n",
    " '听到这个好消息，我感到非常兴奋。']\n",
    "eng_texts = ['Today is a sunny day.',\n",
    " 'This movie is so disappointing, the plot is dull, and the performance is poor.',\n",
    " 'I enjoy spending happy time with friends.',\n",
    " 'The food in this restaurant is delicious, and the service is attentive.',\n",
    " \"Today's workload is heavy, but I feel fulfilled.\",\n",
    " 'The content of this book is very inspiring and enlightening.',\n",
    " 'I find this question difficult to answer and requires more thought.',\n",
    " 'The environment of this city is beautiful and suitable for living.',\n",
    " 'I am reserved about this plan.',\n",
    " 'I am very excited to hear this good news.']\n",
    "senti_labels = ['Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f5d3eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uer/roberta-base-finetuned-dianping-chinese 模型支持语言: 中文 \n",
      "标签  ： ['Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Positive'] \n",
      "中文识别： ['positive (stars 4 and 5)', 'negative (stars 1, 2 and 3)', 'positive (stars 4 and 5)', 'positive (stars 4 and 5)', 'positive (stars 4 and 5)', 'positive (stars 4 and 5)', 'negative (stars 1, 2 and 3)', 'positive (stars 4 and 5)', 'negative (stars 1, 2 and 3)', 'positive (stars 4 and 5)'] \n",
      "预测概率： [0.7619587182998657, 0.9963312745094299, 0.7475177645683289, 0.9657039642333984, 0.8325814604759216, 0.913057804107666, 0.6812158226966858, 0.889066219329834, 0.9668331742286682, 0.5953904390335083] \n",
      "英文识别： ['positive (stars 4 and 5)', 'negative (stars 1, 2 and 3)', 'positive (stars 4 and 5)', 'positive (stars 4 and 5)', 'positive (stars 4 and 5)', 'positive (stars 4 and 5)', 'positive (stars 4 and 5)', 'positive (stars 4 and 5)', 'negative (stars 1, 2 and 3)', 'positive (stars 4 and 5)'] \n",
      "预测概率： [0.9713166356086731, 0.5512093305587769, 0.8332055807113647, 0.912639319896698, 0.5856082439422607, 0.8635313510894775, 0.9521235227584839, 0.9829022884368896, 0.6492404937744141, 0.9080930948257446]\n",
      "ProsusAI/finbert 模型支持语言: 英文 \n",
      "标签  ： ['Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Positive'] \n",
      "中文识别： ['neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral'] \n",
      "预测概率： [0.9235103130340576, 0.9043290019035339, 0.9078810811042786, 0.9074833989143372, 0.9039450287818909, 0.9101026058197021, 0.9034583568572998, 0.9204086661338806, 0.9047182202339172, 0.9110820889472961] \n",
      "英文识别： ['neutral', 'negative', 'neutral', 'neutral', 'neutral', 'neutral', 'negative', 'neutral', 'neutral', 'neutral'] \n",
      "预测概率： [0.714862585067749, 0.9685542583465576, 0.9016146063804626, 0.7124956846237183, 0.4978002905845642, 0.8063870072364807, 0.7696256041526794, 0.8007014989852905, 0.8897315859794617, 0.4929458796977997]\n",
      "lxyuan/distilbert-base-multilingual-cased-sentiments-student 模型支持语言: 多语言 \n",
      "标签  ： ['Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Positive'] \n",
      "中文识别： ['positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'neutral', 'positive'] \n",
      "预测概率： [0.5691527724266052, 0.9656904339790344, 0.9789162278175354, 0.9875038266181946, 0.9210847616195679, 0.9946978092193604, 0.7216320633888245, 0.9830506443977356, 0.4321618378162384, 0.9926959276199341] \n",
      "英文识别： ['positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive'] \n",
      "预测概率： [0.4488370418548584, 0.8975343108177185, 0.9329977631568909, 0.672646164894104, 0.3871454894542694, 0.7364339828491211, 0.3934718370437622, 0.9627594947814941, 0.43074047565460205, 0.9861999750137329]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "models_list = [ {'model_name':'uer/roberta-base-finetuned-dianping-chinese','lang':'中文'},\n",
    "                {'model_name':'ProsusAI/finbert','lang':'英文'},\n",
    "                {'model_name':'lxyuan/distilbert-base-multilingual-cased-sentiments-student','lang':'多语言'}\n",
    "               ]\n",
    "\n",
    "for model in models_list:\n",
    "    pipe = pipeline(\"sentiment-analysis\",model=model['model_name'])\n",
    "    chi_reslut = [ret for ret in pipe(chi_text for chi_text in chi_texts)]\n",
    "    eng_result = [ret for ret in pipe(eng_text for eng_text in eng_texts)]\n",
    "    print(model['model_name'],'模型支持语言:',model['lang'],\n",
    "          '\\n标签  ：',senti_labels,\n",
    "          '\\n中文识别：',[result['label'] for result in chi_reslut],\n",
    "          '\\n预测概率：',[result['score'] for result in chi_reslut],\n",
    "          '\\n英文识别：',[result['label'] for result in eng_result],\n",
    "          '\\n预测概率：',[result['score'] for result in eng_result])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f20ca81",
   "metadata": {},
   "source": [
    "#### 2.NER任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6bac041c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 1.40k/1.40k [00:00<00:00, 4.28MB/s]\n",
      "model.safetensors: 100%|██████████| 990M/990M [02:28<00:00, 6.65MB/s] \n",
      "Some weights of T5ForTokenClassification were not initialized from the model checkpoint at google/flan-t5-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "tokenizer_config.json: 100%|██████████| 2.54k/2.54k [00:00<00:00, 6.48MB/s]\n",
      "spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 14.9MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.42M/2.42M [00:01<00:00, 1.68MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 2.20k/2.20k [00:00<00:00, 7.05MB/s]\n",
      "/root/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:168: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'LABEL_1',\n",
       "  'score': 0.7952455,\n",
       "  'word': 'Hugging Face is',\n",
       "  'start': 0,\n",
       "  'end': 15},\n",
       " {'entity_group': 'LABEL_0',\n",
       "  'score': 0.9329552,\n",
       "  'word': '',\n",
       "  'start': 15,\n",
       "  'end': 16},\n",
       " {'entity_group': 'LABEL_1',\n",
       "  'score': 0.9491247,\n",
       "  'word': 'a',\n",
       "  'start': 16,\n",
       "  'end': 17},\n",
       " {'entity_group': 'LABEL_0',\n",
       "  'score': 0.8633172,\n",
       "  'word': 'French company ',\n",
       "  'start': 17,\n",
       "  'end': 33},\n",
       " {'entity_group': 'LABEL_1',\n",
       "  'score': 0.8657277,\n",
       "  'word': 'based in New',\n",
       "  'start': 33,\n",
       "  'end': 45},\n",
       " {'entity_group': 'LABEL_0',\n",
       "  'score': 0.9488959,\n",
       "  'word': 'York City.',\n",
       "  'start': 45,\n",
       "  'end': 56}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(task=\"ner\", model='google/flan-t5-base',grouped_entities=True)\n",
    "classifier(\"Hugging Face is a French company based in New York City.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8715889",
   "metadata": {},
   "source": [
    "#### 3.对比问答任务\n",
    "构造测试用数据，设置多个比对模型，依次调用，测试问答效果，分别对比各模型的实际效果。模型抽取了排名靠前的若干模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03a295d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "qas = [\n",
    "    {\n",
    "        'question': 'What are the advantages of using deep learning?',\n",
    "        'context': 'Deep learning offers advantages such as automatic feature extraction, scalability to large datasets, and state-of-the-art performance in various tasks.'\n",
    "    },\n",
    "    {\n",
    "        'question': 'How do recurrent neural networks (RNNs) differ from feedforward neural networks?',\n",
    "        'context': 'RNNs have feedback connections that allow them to maintain a state or memory over time, making them suitable for sequential data processing tasks.'\n",
    "    },\n",
    "    {\n",
    "        'question': 'What is the purpose of dropout in neural networks?',\n",
    "        'context': 'Dropout is a regularization technique used in neural networks to prevent overfitting by randomly deactivating neurons during training.'\n",
    "    },\n",
    "    {\n",
    "        'question': 'What are the challenges of training deep neural networks?',\n",
    "        'context': 'Challenges of training deep neural networks include vanishing or exploding gradients, overfitting, and the need for large amounts of labeled data.'\n",
    "    },\n",
    "    {\n",
    "        'question': 'How does the backpropagation algorithm work in neural networks?',\n",
    "        'context': 'Backpropagation is an algorithm used to train neural networks by iteratively adjusting the weights based on the gradient of the loss function with respect to the network parameters.'\n",
    "    },\n",
    "    {\n",
    "        'question': 'What is the role of convolutional layers in convolutional neural networks (CNNs)?',\n",
    "        'context': 'Convolutional layers in CNNs perform feature extraction by applying filters to input data, capturing spatial patterns in images or other structured data.'\n",
    "    },\n",
    "    {\n",
    "        'question': 'How does batch normalization improve training in neural networks?',\n",
    "        'context': 'Batch normalization normalizes the activations of each layer in a neural network, reducing internal covariate shift and allowing for faster and more stable training.'\n",
    "    },\n",
    "    {\n",
    "        'question': 'What are the different types of activation functions used in neural networks?',\n",
    "        'context': 'Common activation functions in neural networks include sigmoid, tanh, ReLU, and softmax, each serving different purposes in network architectures.'\n",
    "    },\n",
    "    {\n",
    "        'question': 'How does attention mechanism work in deep learning models?',\n",
    "        'context': 'Attention mechanism allows models to focus on relevant parts of the input sequence, enhancing performance in tasks such as machine translation and image captioning.'\n",
    "    },\n",
    "    {\n",
    "        'question': 'What is the role of the loss function in training neural networks?',\n",
    "        'context': 'The loss function measures the difference between predicted and actual values, providing feedback to the network during training to adjust the model parameters.'\n",
    "    }\n",
    "]\n",
    "\n",
    "# 单独的答案列表\n",
    "answers = [\n",
    "    'Automatic feature extraction, scalability, state-of-the-art performance',\n",
    "    'RNNs have feedback connections, suitable for sequential data processing',\n",
    "    'Prevent overfitting by deactivating neurons randomly',\n",
    "    'Vanishing or exploding gradients, overfitting, need for large labeled data',\n",
    "    'Iteratively adjusting weights based on gradient of loss function',\n",
    "    'Perform feature extraction by applying filters to input data',\n",
    "    'Normalize activations of each layer, reduce internal covariate shift',\n",
    "    'Sigmoid, tanh, ReLU, softmax',\n",
    "    'Focus on relevant parts of input sequence',\n",
    "    'Measure difference between predicted and actual values'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bdc483c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepset/roberta-base-squad2\n",
      "['Automatic feature extraction, scalability, state-of-the-art performance', 'RNNs have feedback connections, suitable for sequential data processing', 'Prevent overfitting by deactivating neurons randomly', 'Vanishing or exploding gradients, overfitting, need for large labeled data', 'Iteratively adjusting weights based on gradient of loss function', 'Perform feature extraction by applying filters to input data', 'Normalize activations of each layer, reduce internal covariate shift', 'Sigmoid, tanh, ReLU, softmax', 'Focus on relevant parts of input sequence', 'Measure difference between predicted and actual values']\n",
      "['automatic feature extraction, scalability to large datasets', 'feedback connections', 'to prevent overfitting', 'vanishing or exploding gradients, overfitting', 'by iteratively adjusting the weights based on the gradient of the loss function', 'perform feature extraction', 'faster and more stable training', 'sigmoid, tanh, ReLU, and softmax', 'focus on relevant parts of the input sequence', 'providing feedback']\n",
      "[0.017636800184845924, 0.29379376769065857, 0.27211683988571167, 0.0694844126701355, 0.0693860873579979, 0.5598334074020386, 0.3609188199043274, 0.7961347103118896, 0.3116189241409302, 0.3782510459423065]\n",
      "distilbert/distilbert-base-cased-distilled-squad\n",
      "['Automatic feature extraction, scalability, state-of-the-art performance', 'RNNs have feedback connections, suitable for sequential data processing', 'Prevent overfitting by deactivating neurons randomly', 'Vanishing or exploding gradients, overfitting, need for large labeled data', 'Iteratively adjusting weights based on gradient of loss function', 'Perform feature extraction by applying filters to input data', 'Normalize activations of each layer, reduce internal covariate shift', 'Sigmoid, tanh, ReLU, softmax', 'Focus on relevant parts of input sequence', 'Measure difference between predicted and actual values']\n",
      "['automatic feature extraction, scalability to large datasets', 'feedback connections', 'to prevent overfitting', 'vanishing or exploding gradients', 'adjusting the weights', 'feature extraction', 'faster and more stable', 'sigmoid, tanh, ReLU, and softmax', 'enhancing performance', 'providing feedback']\n",
      "[0.18405120074748993, 0.6112111806869507, 0.3641989231109619, 0.0192941315472126, 0.0743497684597969, 0.5401722192764282, 0.2813470959663391, 0.9898022413253784, 0.28231436014175415, 0.39277705550193787]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "models_list = [ {'model_name':'deepset/roberta-base-squad2','tag':'趋势排行榜第一'},\n",
    "                {'model_name':'distilbert/distilbert-base-cased-distilled-squad','tag':'下载量第二'},\n",
    "               ]\n",
    "\n",
    "for model in models_list:\n",
    "    \n",
    "    pipe = pipeline(\"question-answering\",model=model['model_name'])\n",
    "    \n",
    "    ans = [ans for ans in pipe(qa for qa in qas)]\n",
    "\n",
    "    print(model['model_name'])\n",
    "    print(answers)\n",
    "    print([a['answer'] for a in ans])\n",
    "    print([a['score'] for a in ans])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdd5627",
   "metadata": {},
   "source": [
    "#### 任务4 总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d143503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sshleifer/distilbart-cnn-12-6\n",
      "[[{'summary_text': ' Whiskers: \"Whiskers\" \\xa0- \\xa0’�:\\xa0‘I’ll always be happy to have your friends with me.’��'}], [{'summary_text': ' Whiskers chased a sandwich away from a picnic table by a river in a small village . The cat was about to sink his teeth into the tasty treat, but a gust of wind swept through the picnic area, sending it tumbling into the river .'}]]\n",
      "philschmid/bart-large-cnn-samsum\n",
      "[[{'summary_text': 'Whiskers are cute little animals.'}], [{'summary_text': 'Whiskers the cat lives in a small village. One day he saw a group of villagers preparing for a picnic by the river. He followed them and snatched a sandwich from the edge of the blanket, but a gust of wind knocked it into the river and he had to fight the current to retrieve it. The villagers rescued him.'}]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "chi_texts = '故事背景是一个小村庄，生活着一只名叫Whiskers的调皮猫。Whiskers不同于其他猫，他有一种冒险精神和无法满足的好奇心。一个阳光明媚的下午，他发现村民们在准备野餐，他跟着偷偷尾随。村民们在河边野餐，Whiskers看着美味的食物，忍不住偷了一块三明治。但是被一阵风吹走，飞到了河里。Whiskers毫不犹豫地跳了进去，不顾危险地想要拯救三明治。他不断地向前游，终于抓住了三明治。但是他被水冲走了，被村民救了出来。村民为他的勇敢欢呼，尊重他。Whiskers虽然失去了三明治，但赢得了村民的尊敬。'\n",
    "\n",
    "eng_texts = 'Once upon a time, in a small village nestled between rolling hills, there lived a mischievous cat named Whiskers. Whiskers was unlike any other cat in the village. He had a knack for adventure and an insatiable curiosity that often led him into trouble.One sunny afternoon, Whiskers spotted a group of villagers preparing for a picnic by the tranquil river that flowed through the heart of the village. Unable to resist the temptation of delicious food, Whiskers stealthily followed them, his tail twitching with excitement.As the villagers settled down for their picnic, Whiskers watched from a distance, his eyes fixated on the mouthwatering spread of sandwiches, fruits, and pastries. Unable to control his hunger any longer, Whiskers sprang into action, darting forward to snatch a sandwich from the edge of the blanket.But just as he was about to sink his teeth into the tasty treat, a mischievous gust of wind swept through the picnic area, carrying the sandwich away and sending it tumbling into the river. Without hesitation, Whiskers leaped into action, determined to retrieve his stolen prize.As he paddled furiously against the current, Whiskers realized that he had underestimated the power of the river. The currents were strong, and the sandwich seemed to be moving further away with each passing moment. But Whiskers refused to give up. With a burst of determination, he pushed himself harder, his paws slicing through the water as he chased after the elusive sandwich.After what felt like an eternity, Whiskers finally caught up to the sandwich, his heart pounding with triumph. But just as he reached out to grab it, he felt a pair of strong hands scoop him out of the water. It was the villagers, who had seen his daring rescue mission and rushed to save him.As Whiskers lay panting on the riverbank, the villagers laughed and cheered, amazed by his bravery. And though the sandwich was lost forever, Whiskers had gained something far more valuable – the respect and admiration of the entire village.'\n",
    "\n",
    "models_list = [ {'model_name':'sshleifer/distilbart-cnn-12-6','tag':'下载榜第二'},\n",
    "                {'model_name':'philschmid/bart-large-cnn-samsum','tag':'下载量第三'},\n",
    "               ]\n",
    "\n",
    "for model in models_list:\n",
    "    \n",
    "    pipe =  pipeline(task=\"summarization\",\n",
    "                      model=model['model_name'],\n",
    "                      min_length=8,\n",
    "                      max_length=100,)\n",
    "    \n",
    "    summary = [pipe(chi_texts),pipe(eng_texts)]\n",
    "\n",
    "    print(model['model_name'])\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50b9674",
   "metadata": {},
   "source": [
    "#### 任务5.音频分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4fe570d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/audio_utils.py:198: UserWarning: At least one mel filter has all zero values. The value for `num_mel_filters` (128) may be set too high. Or, the value for `num_frequency_bins` (256) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIT/ast-finetuned-audioset-10-10-0.4593 [{'score': 0.4207741916179657, 'label': 'Speech'}, {'score': 0.1793096661567688, 'label': 'Rain on surface'}, {'score': 0.13007479906082153, 'label': 'Rain'}, {'score': 0.09596990048885345, 'label': 'Raindrop'}, {'score': 0.057824958115816116, 'label': 'Music'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 2.19k/2.19k [00:00<00:00, 6.87MB/s]\n",
      "model.safetensors: 100%|██████████| 1.26G/1.26G [05:20<00:00, 3.94MB/s]\n",
      "Some weights of the model checkpoint at alefiury/wav2vec2-large-xlsr-53-gender-recognition-librispeech were not used when initializing Wav2Vec2ForSequenceClassification: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at alefiury/wav2vec2-large-xlsr-53-gender-recognition-librispeech and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "preprocessor_config.json: 100%|██████████| 212/212 [00:00<00:00, 781kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alefiury/wav2vec2-large-xlsr-53-gender-recognition-librispeech [{'score': 0.9985793828964233, 'label': 'male'}, {'score': 0.0014206045307219028, 'label': 'female'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "models_list = [ {'model_name':'MIT/ast-finetuned-audioset-10-10-0.4593','tag':'下载量第一'},\n",
    "                {'model_name':'alefiury/wav2vec2-large-xlsr-53-gender-recognition-librispeech','tag':'下载量第3'},\n",
    "               ]\n",
    "\n",
    "\n",
    "for model in models_list:\n",
    "    \n",
    "    pipe =  pipeline(task=\"audio-classification\",\n",
    "                      model=model['model_name'])\n",
    "    \n",
    "    preds = pipe(\"data/audio/mlk.flac\")\n",
    "\n",
    "    print(model['model_name'],preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d198dbc",
   "metadata": {},
   "source": [
    "#### 6.语音识别转文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7961b36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 1.98k/1.98k [00:00<00:00, 5.66MB/s]\n",
      "model.safetensors: 100%|██████████| 151M/151M [00:58<00:00, 2.60MB/s] \n",
      "generation_config.json: 100%|██████████| 3.75k/3.75k [00:00<00:00, 10.8MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 805/805 [00:00<00:00, 2.24MB/s]\n",
      "vocab.json: 100%|██████████| 836k/836k [00:01<00:00, 598kB/s]\n",
      "tokenizer.json: 100%|██████████| 2.48M/2.48M [00:00<00:00, 3.63MB/s]\n",
      "merges.txt: 100%|██████████| 494k/494k [00:00<00:00, 3.60MB/s]\n",
      "normalizer.json: 100%|██████████| 52.7k/52.7k [00:00<00:00, 286kB/s]\n",
      "added_tokens.json: 100%|██████████| 34.6k/34.6k [00:00<00:00, 48.6MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 2.08k/2.08k [00:00<00:00, 5.51MB/s]\n",
      "preprocessor_config.json: 100%|██████████| 185k/185k [00:01<00:00, 106kB/s]\n",
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai/whisper-tiny {'text': ' I have a dream. Good one day. This nation will rise up. Live out the true meaning of its dream.'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "models_list = [ {'model_name':'openai/whisper-tiny','tag':'whisper'},\n",
    "               ]\n",
    "\n",
    "\n",
    "for model in models_list:\n",
    "    \n",
    "    pipe =  pipeline(task=\"automatic-speech-recognition\",\n",
    "                      model=model['model_name'])\n",
    "    \n",
    "    text = pipe(\"data/audio/mlk.flac\")\n",
    "\n",
    "    print(model['model_name'],text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eb6216",
   "metadata": {},
   "source": [
    "#### 6.图像分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d67868df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.5874, 'label': 'lynx, catamount'}\n",
      "{'score': 0.1289, 'label': 'tabby, tabby cat'}\n",
      "{'score': 0.075, 'label': 'marmot'}\n",
      "{'score': 0.0382, 'label': 'badger'}\n",
      "{'score': 0.0131, 'label': 'Egyptian cat'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 70.0k/70.0k [00:00<00:00, 118kB/s]\n",
      "pytorch_model.bin: 100%|██████████| 22.5M/22.5M [00:09<00:00, 2.27MB/s]\n",
      "preprocessor_config.json: 100%|██████████| 187/187 [00:00<00:00, 579kB/s]\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9441, 'label': 'lynx, catamount'}\n",
      "{'score': 0.0041, 'label': 'tiger cat'}\n",
      "{'score': 0.0036, 'label': 'grey fox, gray fox, Urocyon cinereoargenteus'}\n",
      "{'score': 0.0026, 'label': 'Egyptian cat'}\n",
      "{'score': 0.002, 'label': 'tabby, tabby cat'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "models_list = [ {'model_name':'microsoft/resnet-50','tag':'mostlike2'},\n",
    "                {'model_name':'apple/mobilevit-small','tag':'mostlike4'},\n",
    "               ]\n",
    "for model in models_list:\n",
    "\n",
    "    classifier = pipeline(task=\"image-classification\",model=model['model_name'])\n",
    "\n",
    "    preds = classifier(\n",
    "        \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n",
    "    )\n",
    "    preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n",
    "    print(*preds, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1398a40",
   "metadata": {},
   "source": [
    "#### 7.目标检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7861c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: timm in /root/miniconda3/envs/llm/lib/python3.10/site-packages (0.9.16)\n",
      "Requirement already satisfied: torch in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from timm) (2.2.1)\n",
      "Requirement already satisfied: torchvision in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from timm) (0.17.1)\n",
      "Requirement already satisfied: pyyaml in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from timm) (6.0.1)\n",
      "Requirement already satisfied: huggingface_hub in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from timm) (0.20.3)\n",
      "Requirement already satisfied: safetensors in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from timm) (0.4.2)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from huggingface_hub->timm) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from huggingface_hub->timm) (2023.10.0)\n",
      "Requirement already satisfied: requests in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from huggingface_hub->timm) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from huggingface_hub->timm) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from huggingface_hub->timm) (4.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from huggingface_hub->timm) (23.1)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch->timm) (1.12)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch->timm) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch->timm) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch->timm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch->timm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch->timm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch->timm) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch->timm) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch->timm) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch->timm) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch->timm) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch->timm) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch->timm) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch->timm) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch->timm) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->timm) (12.3.101)\n",
      "Requirement already satisfied: numpy in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torchvision->timm) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torchvision->timm) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from jinja2->torch->timm) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from sympy->torch->timm) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec1b71a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9946,\n",
       "  'label': 'cat',\n",
       "  'box': {'xmin': 75, 'ymin': 60, 'xmax': 290, 'ymax': 369}},\n",
       " {'score': 0.9899,\n",
       "  'label': 'dog',\n",
       "  'box': {'xmin': 280, 'ymin': 18, 'xmax': 479, 'ymax': 416}}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "detector = pipeline(task=\"object-detection\",model='hustvl/yolos-tiny')\n",
    "\n",
    "preds = detector(\n",
    "    \"data/image/cat_dog.jpg\"\n",
    ")\n",
    "preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"], \"box\": pred[\"box\"]} for pred in preds]\n",
    "preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
